{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Apriori.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxyQIqlGOzYV"
      },
      "outputs": [],
      "source": [
        "def apriori(dataset, min_support=0.5, verbose=False):\n",
        "  C1 = create_candidate(dataset)\n",
        "  D = map(set, dataset)\n",
        "  F1, support_data = support_prune(D, C1, min_support,verbose=False)\n",
        "  F = [F1]    #list of frequent itemsets, initialized to frequent 1-itemsets\n",
        "  k = 2   #itemset cardinality\n",
        "  while (len(F[k-2])>0):\n",
        "    Ck = apriori_gen(F[k-2], k)   #generate candidate itemset\n",
        "    Fk, supK = support_prune(D, Ck, min_support)  #prune candidate itemset \n",
        "    support_data.update(supK)   #update the support counts to reflect pruning \n",
        "    F.append(Fk)  #add the pruned candidate itemsets to the list of frequent itemset \n",
        "    k += 1 \n",
        "\n",
        "    if verbose:\n",
        "      #print a list of the frequent itemsets \n",
        "      for kset in F:\n",
        "        for item in kset:\n",
        "          print(\"\"\\\n",
        "                + \"{\"\\\n",
        "                +\"\".join(str(i) + \",\" for i in iter(item).rstrip(',')\\\n",
        "                + \"}\"\\\n",
        "                + \": sup = \" + str(round(support_data[item], 3))))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_candidates(dataset, verbose=False):\n",
        "  c1 = []   #list of all items in the database of transactions \n",
        "  for transaction in dataset:\n",
        "    for item in transaction:\n",
        "      if not [item] in c1:\n",
        "        c1.append([item])\n",
        "  c1.sort()\n",
        "\n",
        "  if verbose:\n",
        "    #print a list of all the candidate items \n",
        "    print(\"\" \\\n",
        "            + \"{\" \\\n",
        "            + \"\".join(str(i[0]) + \", \" for i in iter(c1)).rstrip(', ') \\\n",
        "            + \"}\")\n",
        "    #map c1 to a frozenset bec it will be the key of a dictionary \n",
        "    return map(frozenset, c1)"
      ],
      "metadata": {
        "id": "cr_xoGKYtWxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def support_prune(dataset, candidates, min_support, verbose=False):\n",
        "  sscnt = {}    #set for support counts \n",
        "  for tid in dataset:\n",
        "    for can in candidates:\n",
        "      if can.issubset(tid):\n",
        "        sscnt.setdefault(can, 0)\n",
        "        sscnt[can] += 1\n",
        "  num_items = float(len(dataset))   #total number of transactions in the dataset \n",
        "  retlist = []  #array for unpruned itemsets \n",
        "  support_data = {}   #set for support data for corresponding itemsets \n",
        "  for key in sscnt:\n",
        "    #calculate the support of itemset key \n",
        "    support = sscnt[key] /num_items\n",
        "    if support >= min_support:\n",
        "      retlist.insert(0, key)\n",
        "    support_data[key] = support \n",
        "\n",
        "    #print a list of the pruned itemsets \n",
        "    if verbose:\n",
        "      for kset in retlist:\n",
        "        for item in kset:\n",
        "          print(\"{\" + str(item) + \"}\")\n",
        "          print(\"\")\n",
        "\n",
        "          for key in sscnt:\n",
        "            print(\"\" \\\n",
        "                  + \"{\"\\\n",
        "                  + \"\".join([str(item) + \"}\"\n",
        "                  + \"}\" \\\n",
        "                  + \": sup = \" + str(support_data[key])]))\n",
        "    return retlist, support_data"
      ],
      "metadata": {
        "id": "PEXm0WtgtW0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apriori_gen(freq_sets, k):\n",
        "  retList = []  #list of merged frequent itemsets \n",
        "  lenLk = len(freq_sets)  #number of frequent itemsets \n",
        "  for i in range(i+1, lenLk):\n",
        "    a = list(freq_sets[i])\n",
        "    b = list(freq_sets[j])\n",
        "    a.sort()\n",
        "    b.sort()\n",
        "    F1 = a[:k-2]  #first k-2 items of freq_sets[i]\n",
        "    F2 = b[:k-2]  #first k-2 items of freq_sets[j]\n",
        "\n",
        "    if F1 == F2:\n",
        "      #if the first k-2 items are identical \n",
        "      #merge the freq itemsets \n",
        "      retList.append(freq_sets[i] | freq_sets[j])\n",
        "    return retList"
      ],
      "metadata": {
        "id": "J9_bovuZwPRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rules_from_conseq(freq_set, H, support_data, rules, min_confidence=0.5, verbose=False):\n",
        "  m = len(H[0])\n",
        "  if m == 1:\n",
        "        Hmp1 = calc_confidence(freq_set, H, support_data, rules, min_confidence, verbose)\n",
        "  if (len(freq_set) > (m+1)):\n",
        "        Hmp1 = apriori_gen(H, m+1) # generate candidate itemsets\n",
        "        Hmp1 = calc_confidence(freq_set, Hmp1, support_data, rules, min_confidence, verbose)\n",
        "        if len(Hmp1) > 1:\n",
        "            # If there are candidate rules above the minimum confidence \n",
        "            # threshold, recurse on the list of these candidate rules.\n",
        "            rules_from_conseq(freq_set, Hmp1, support_data, rules, min_confidence, verbose)\n",
        "\n",
        "def calc_confidence(freq_set, H, support_data, rules, min_confidence=0.5, verbose=False):\n",
        "    pruned_H = [] # list of candidate rules above the minimum confidence threshold\n",
        "    for conseq in H: # iterate over the frequent itemsets\n",
        "        conf = support_data[freq_set] / support_data[freq_set - conseq]\n",
        "        if conf >= min_confidence:\n",
        "            rules.append((freq_set - conseq, conseq, conf))\n",
        "            pruned_H.append(conseq)\n",
        "\n",
        "            if verbose:\n",
        "                print(\"\" \\\n",
        "                    + \"{\" \\\n",
        "                    + \"\".join([str(i) + \", \" for i in iter(freq_set-conseq)]).rstrip(', ') \\\n",
        "                    + \"}\" \\\n",
        "                    + \" ---> \" \\\n",
        "                    + \"{\" \\\n",
        "                    + \"\".join([str(i) + \", \" for i in iter(conseq)]).rstrip(', ') \\\n",
        "                    + \"}\" \\\n",
        "                    + \":  conf = \" + str(round(conf, 3)) \\\n",
        "                    + \", sup = \" + str(round(support_data[freq_set], 3)))\n",
        "\n",
        "    return pruned_H"
      ],
      "metadata": {
        "id": "K5bp-V2-wPTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_rules(F, support_data, min_confidence=0.5, verbose=True):\n",
        "    rules = []\n",
        "    for i in range(1, len(F)):\n",
        "        for freq_set in F[i]:\n",
        "            H1 = [frozenset([itemset]) for itemset in freq_set]\n",
        "            if (i > 1):\n",
        "                rules_from_conseq(freq_set, H1, support_data, rules, min_confidence, verbose)\n",
        "            else:\n",
        "                calc_confidence(freq_set, H1, support_data, rules, min_confidence, verbose)\n",
        "\n",
        "    return rules"
      ],
      "metadata": {
        "id": "r1vo2IrHz05q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Gg15oPl9wPWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "\n",
        "def load_dataset():\n",
        "    \"\"\"Loads an example of market basket transactions for testing purposes.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A list (database) of lists (transactions). Each element of a transaction \n",
        "    is an item.\n",
        "    \"\"\"\n",
        "    return [['Bread', 'Milk'], \n",
        "            ['Bread', 'Diapers', 'Beer', 'Eggs'], \n",
        "            ['Milk', 'Diapers', 'Beer', 'Coke'], \n",
        "            ['Bread', 'Milk', 'Diapers', 'Beer'], \n",
        "            ['Bread', 'Milk', 'Diapers', 'Coke']]\n",
        "\n",
        "dataset = load_dataset() # list of transactions; each transaction is a list of items\n",
        "D = map(set, dataset) # set of transactions; each transaction is a list of items\n",
        "\n",
        "pprint.pprint(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3zO2lmBwPYv",
        "outputId": "11305035-18b3-4342-fd6f-60c1ecfbe992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Bread', 'Milk'],\n",
            " ['Bread', 'Diapers', 'Beer', 'Eggs'],\n",
            " ['Milk', 'Diapers', 'Beer', 'Coke'],\n",
            " ['Bread', 'Milk', 'Diapers', 'Beer'],\n",
            " ['Bread', 'Milk', 'Diapers', 'Coke']]\n"
          ]
        }
      ]
    }
  ]
}